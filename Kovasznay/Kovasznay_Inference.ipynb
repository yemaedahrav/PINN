{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfUgr12TxZwj",
    "outputId": "d2dcd59e-3a84-4527-fbc1-3f7f1c65903e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameyv\\anaconda3\\envs\\amey-pinn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyDOE in c:\\users\\ameyv\\anaconda3\\envs\\amey-pinn\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\ameyv\\anaconda3\\envs\\amey-pinn\\lib\\site-packages (from pyDOE) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\ameyv\\anaconda3\\envs\\amey-pinn\\lib\\site-packages (from pyDOE) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "!pip install pyDOE\n",
    "from pyDOE import lhs\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kiB7n-jxxrBq"
   },
   "outputs": [],
   "source": [
    "# Multi-layer Perceptron (Deep Neural Network)\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        self.depth = len(layers) - 1\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1])))\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3kURWSzsx5MQ"
   },
   "outputs": [],
   "source": [
    "# Physics Informed Neural Network\n",
    "class PINN():\n",
    "    def __init__(self, X_s, s, X_f, layers, lb, ub, nu):\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        # data\n",
    "        self.x_s = torch.tensor(X_s[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.y_s = torch.tensor(X_s[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.y_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.s = torch.tensor(s).float().to(device)\n",
    "        \n",
    "        # deep neural networks\n",
    "        self.layers = layers\n",
    "        self.nu = nu\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        \n",
    "        # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"      \n",
    "        )\n",
    "        self.iter = 0\n",
    "\n",
    "    def net_s(self, x, y):  \n",
    "        s = self.dnn(torch.cat([x, y], dim=1))\n",
    "        return s\n",
    "    \n",
    "    def net_f(self, x, y):\n",
    "        s = self.net_s(x, y)\n",
    "        u = s[:,0]\n",
    "        v = s[:,1]\n",
    "        p = s[:,2]\n",
    "\n",
    "        du_dx = torch.autograd.grad(inputs=x, outputs=u, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dx = torch.autograd.grad(inputs=x, outputs=v, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
    "        dp_dx = torch.autograd.grad(inputs=x, outputs=p, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        du_dy = torch.autograd.grad(inputs=y, outputs=u, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dy = torch.autograd.grad(inputs=y, outputs=v, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
    "        dp_dy = torch.autograd.grad(inputs=y, outputs=p, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        du_dxx = torch.autograd.grad(inputs=x, outputs=du_dx, grad_outputs=torch.ones_like(du_dx), retain_graph=True, create_graph=True)[0]\n",
    "        du_dyy = torch.autograd.grad(inputs=y, outputs=du_dy, grad_outputs=torch.ones_like(du_dy), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dxx = torch.autograd.grad(inputs=x, outputs=dv_dx, grad_outputs=torch.ones_like(dv_dx), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dyy = torch.autograd.grad(inputs=y, outputs=dv_dy, grad_outputs=torch.ones_like(dv_dy), retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        f1 = u.squeeze() * du_dx + v.squeeze() * du_dy + dp_dx - self.nu * (du_dxx + du_dyy)\n",
    "        f2 = u.squeeze() * dv_dx + v.squeeze() * dv_dy + dp_dy - self.nu * (dv_dxx + dv_dyy)\n",
    "        f = f1 + f2\n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        s_pred = self.net_s(self.x_s, self.y_s)\n",
    "        f_pred = self.net_f(self.x_f, self.y_f)\n",
    "#         print(\"s:\", type(self.s))\n",
    "#         print(\"f_pred:\", type(f_pred))\n",
    "#         print(\"s_pred:\", type(s_pred))\n",
    "        loss_s = torch.mean((self.s - s_pred) ** 2)\n",
    "        loss_f = torch.mean(f_pred ** 2)\n",
    "        \n",
    "        loss = loss_s + loss_f\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print('Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_s.item(), loss_f.item()))\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_func)\n",
    "     \n",
    "    def predict(self, X):\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        \n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        y = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        s = self.net_s(x, y)\n",
    "        f = self.net_f(x, y)\n",
    "        s = s.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return s, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtUzPSDH2XWf",
    "outputId": "fabe0ed9-d342-4ef4-95b0-a4cffc540c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([200])\n",
      "y: torch.Size([200])\n",
      "X: torch.Size([200, 200])\n",
      "Y: torch.Size([200, 200])\n",
      "Exact: torch.Size([200, 200, 3])\n",
      "X_test: (40000, 2)\n",
      "s_test: torch.Size([40000, 3])\n",
      "X_s_train: (500, 2)\n",
      "s_train: torch.Size([500, 3])\n",
      "X_f_train: (5500, 2)\n"
     ]
    }
   ],
   "source": [
    "N_u = 500\n",
    "N_f = 5000\n",
    "\n",
    "Re = 20\n",
    "nu = 1 / Re\n",
    "lamb = 1 / (2 * nu) - np.sqrt(1 / (4 * nu ** 2) + 4 * np.pi ** 2)\n",
    "\n",
    "layers = [2, 30, 30, 30, 30, 30, 30, 3]\n",
    "\n",
    "h = 0.005\n",
    "k = 0.005\n",
    "x = torch.arange(0, 1, h)\n",
    "y = torch.arange(0, 1, k)\n",
    "\n",
    "print(\"x:\", x.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "X, Y = np.meshgrid(x,y)\n",
    "X = torch.from_numpy(X)\n",
    "Y = torch.from_numpy(Y)\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"Y:\", Y.shape)\n",
    "\n",
    "Exact = torch.zeros(X.shape[0],X.shape[1],3)\n",
    "Exact_U = 1-(torch.exp(-lamb*X))*(torch.cos(2*math.pi*Y))\n",
    "Exact_V = (lamb/(2*math.pi))*(torch.exp(lamb*X))*(torch.sin(2*math.pi*X))\n",
    "Exact_P = (1/2)*(1-(torch.exp(2*lamb*X)))\n",
    "Exact[:,:,0] = Exact_U\n",
    "Exact[:,:,1] = Exact_V\n",
    "Exact[:,:,2] = Exact_P\n",
    "\n",
    "print(\"Exact:\",Exact.shape)\n",
    "\n",
    "X_test = np.hstack((X.flatten()[:,None], Y.flatten()[:,None]))\n",
    "s_test = torch.reshape(Exact, (-1, 3))             \n",
    "\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"s_test:\", s_test.shape)\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_test.min(0)\n",
    "ub = X_test.max(0) \n",
    "\n",
    "\n",
    "idx = np.random.choice(1000, N_u, replace=False)\n",
    "X_s_train = X_test[idx, :]\n",
    "s_train   = s_test[idx, :]\n",
    "\n",
    "print(\"X_s_train:\", X_s_train.shape)\n",
    "print(\"s_train:\", s_train.shape)\n",
    "\n",
    "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_s_train))\n",
    "\n",
    "print(\"X_f_train:\", X_f_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "N4wu_azTH_WP",
    "outputId": "45a0cd76-490b-4992-cd76-48f76a323d0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameyv\\AppData\\Local\\Temp\\ipykernel_5984\\1660453502.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.s = torch.tensor(s).float().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100, Loss: 1.35593e-01, Loss_u: 1.10042e-01, Loss_f: 2.55512e-02\n",
      "Iter 200, Loss: 9.08227e-02, Loss_u: 8.46753e-02, Loss_f: 6.14740e-03\n",
      "Iter 300, Loss: 8.73745e-02, Loss_u: 8.52080e-02, Loss_f: 2.16652e-03\n",
      "Iter 400, Loss: 8.50999e-02, Loss_u: 8.20708e-02, Loss_f: 3.02906e-03\n",
      "Iter 500, Loss: 8.42041e-02, Loss_u: 8.19362e-02, Loss_f: 2.26789e-03\n",
      "Iter 600, Loss: 8.29125e-02, Loss_u: 8.04520e-02, Loss_f: 2.46049e-03\n",
      "Iter 700, Loss: 8.17817e-02, Loss_u: 7.80997e-02, Loss_f: 3.68208e-03\n",
      "Iter 800, Loss: 8.04997e-02, Loss_u: 7.48100e-02, Loss_f: 5.68974e-03\n",
      "Iter 900, Loss: 7.87158e-02, Loss_u: 6.98795e-02, Loss_f: 8.83627e-03\n",
      "Iter 1000, Loss: 7.66567e-02, Loss_u: 6.79481e-02, Loss_f: 8.70854e-03\n",
      "Iter 1100, Loss: 7.24238e-02, Loss_u: 6.64732e-02, Loss_f: 5.95060e-03\n",
      "Iter 1200, Loss: 6.72201e-02, Loss_u: 6.27092e-02, Loss_f: 4.51085e-03\n",
      "Iter 1300, Loss: 6.46569e-02, Loss_u: 6.07949e-02, Loss_f: 3.86198e-03\n",
      "Iter 1400, Loss: 6.08159e-02, Loss_u: 5.58646e-02, Loss_f: 4.95133e-03\n",
      "Iter 1500, Loss: 5.78078e-02, Loss_u: 5.33308e-02, Loss_f: 4.47700e-03\n",
      "Iter 1600, Loss: 5.52751e-02, Loss_u: 5.18572e-02, Loss_f: 3.41790e-03\n",
      "Iter 1700, Loss: 5.22682e-02, Loss_u: 4.80136e-02, Loss_f: 4.25455e-03\n",
      "Iter 1800, Loss: 5.00587e-02, Loss_u: 4.61924e-02, Loss_f: 3.86637e-03\n",
      "Iter 1900, Loss: 4.81273e-02, Loss_u: 4.44231e-02, Loss_f: 3.70419e-03\n",
      "Iter 2000, Loss: 4.64426e-02, Loss_u: 4.27719e-02, Loss_f: 3.67072e-03\n",
      "Iter 2100, Loss: 4.37303e-02, Loss_u: 4.01580e-02, Loss_f: 3.57224e-03\n",
      "Iter 2200, Loss: 4.22459e-02, Loss_u: 3.84127e-02, Loss_f: 3.83323e-03\n",
      "Iter 2300, Loss: 3.99507e-02, Loss_u: 3.61571e-02, Loss_f: 3.79360e-03\n",
      "Iter 2400, Loss: 3.80520e-02, Loss_u: 3.56829e-02, Loss_f: 2.36905e-03\n",
      "Iter 2500, Loss: 3.68502e-02, Loss_u: 3.46579e-02, Loss_f: 2.19225e-03\n",
      "Iter 2600, Loss: 3.59715e-02, Loss_u: 3.37913e-02, Loss_f: 2.18018e-03\n",
      "Iter 2700, Loss: 3.49097e-02, Loss_u: 3.32935e-02, Loss_f: 1.61625e-03\n"
     ]
    }
   ],
   "source": [
    "model = PINN(X_s_train, s_train, X_f_train, layers, lb, ub, nu)\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
